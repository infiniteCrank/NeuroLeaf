import fs from "fs";
import { parse } from "csv-parse/sync";
import { ELM } from "../src/core/ELM";
import { ELMChain } from "../src/core/ELMChain";
import { UniversalEncoder } from "../src/preprocessing/UniversalEncoder";
import { TFIDFVectorizer } from "../src/core/TFIDF";
import { EmbeddingRecord } from "../src/core/EmbeddingStore";

// âœ… L2 normalization helper
function l2normalize(v: number[]): number[] {
    const norm = Math.sqrt(v.reduce((s, x) => s + x * x, 0));
    return norm === 0 ? v : v.map(x => x / norm);
}

// âœ… Load corpus
const rawText = fs.readFileSync("../public/go_textbook.md", "utf8");

// âœ… Split paragraphs
const paragraphs = rawText
    .split(/\n{2,}/)
    .map(p => p.trim())
    .filter(p => p && p.length > 30);

console.log(`âœ… Parsed ${paragraphs.length} paragraphs.`);

// âœ… Encoder
const encoder = new UniversalEncoder({
    maxLen: 100,
    charSet: "abcdefghijklmnopqrstuvwxyz0123456789",
    mode: "token",
    useTokenizer: true
});

// âœ… Encode paragraphs
const paraVectors = paragraphs.map(p =>
    encoder.normalize(encoder.encode(p))
);

// âœ… Load supervised pairs
const supervisedPaths = [
    "../public/supervised_pairs.csv",
    "../public/supervised_pairs_2.csv",
    "../public/supervised_pairs_3.csv",
    "../public/supervised_pairs_4.csv"
];
let supervisedPairs: { query: string, target: string }[] = [];

for (const path of supervisedPaths) {
    if (fs.existsSync(path)) {
        const csv = fs.readFileSync(path, "utf8");
        const rows = parse(csv, { skip_empty_lines: true });
        supervisedPairs.push(
            ...rows.map((row: [string, string]) => ({ query: row[0].trim(), target: row[1].trim() }))
        );
    }
}

console.log(`âœ… Loaded ${supervisedPairs.length} supervised pairs.`);

const supQueryVecs = supervisedPairs.map(p =>
    encoder.normalize(encoder.encode(p.query))
);
const supTargetVecs = supervisedPairs.map(p =>
    encoder.normalize(encoder.encode(p.target))
);

// âœ… Load negative pairs
const negativePaths = [
    "../public/negative_pairs.csv",
    "../public/negative_pairs_2.csv",
    "../public/negative_pairs_3.csv",
    "../public/negative_pairs_4.csv"
];
let negativePairs: { query: string, target: string }[] = [];

for (const path of negativePaths) {
    if (fs.existsSync(path)) {
        const csv = fs.readFileSync(path, "utf8");
        const rows = parse(csv, { skip_empty_lines: true });
        negativePairs.push(
            ...rows.map((row: [string, string]) => ({ query: row[0].trim(), target: row[1].trim() }))
        );
    }
}

console.log(`âœ… Loaded ${negativePairs.length} negative pairs.`);

const negQueryVecs = negativePairs.map(p =>
    encoder.normalize(encoder.encode(p.query))
);
const negTargetVecs = negativePairs.map(p =>
    encoder.normalize(encoder.encode(p.target))
);

// âœ… Unsupervised ELM
const unsupELM = new ELM({
    activation: "relu",
    hiddenUnits: 128,
    maxLen: paraVectors[0].length,
    categories: [],
    log: { modelName: "UnsupervisedELM", verbose: true },
    dropout: 0.02
});
const unsupPath = "./elm_weights/unsupervised.json";
if (fs.existsSync(unsupPath)) {
    unsupELM.loadModelFromJSON(fs.readFileSync(unsupPath, "utf-8"));
    console.log(`âœ… Loaded Unsupervised ELM weights.`);
} else {
    console.log(`âš™ï¸ Training Unsupervised ELM...`);
    unsupELM.trainFromData(paraVectors, paraVectors);
    fs.writeFileSync(unsupPath, JSON.stringify(unsupELM.model));
    console.log(`ðŸ’¾ Saved Unsupervised ELM weights.`);
}

// âœ… Supervised ELM
const supELM = new ELM({
    activation: "relu",
    hiddenUnits: 128,
    maxLen: supQueryVecs[0].length,
    categories: [],
    log: { modelName: "SupervisedELM", verbose: true },
    dropout: 0.02
});
const supPath = "./elm_weights/supervised.json";
if (fs.existsSync(supPath)) {
    supELM.loadModelFromJSON(fs.readFileSync(supPath, "utf-8"));
    console.log(`âœ… Loaded Supervised ELM weights.`);
} else {
    console.log(`âš™ï¸ Training Supervised ELM...`);
    supELM.trainFromData(supQueryVecs, supTargetVecs);
    fs.writeFileSync(supPath, JSON.stringify(supELM.model));
    console.log(`ðŸ’¾ Saved Supervised ELM weights.`);
}

// âœ… Negative ELM
const negELM = new ELM({
    activation: "relu",
    hiddenUnits: 128,
    maxLen: negQueryVecs[0].length,
    categories: [],
    log: { modelName: "NegativeELM", verbose: true },
    dropout: 0.02
});
const negPath = "./elm_weights/negative.json";
if (fs.existsSync(negPath)) {
    negELM.loadModelFromJSON(fs.readFileSync(negPath, "utf-8"));
    console.log(`âœ… Loaded Negative ELM weights.`);
} else {
    console.log(`âš™ï¸ Training Negative ELM...`);
    negELM.trainFromData(negQueryVecs, negTargetVecs);
    fs.writeFileSync(negPath, JSON.stringify(negELM.model));
    console.log(`ðŸ’¾ Saved Negative ELM weights.`);
}

// âœ… Compute paragraph embeddings
const unsupEmb = unsupELM.computeHiddenLayer(paraVectors).map(l2normalize);

// âœ… TFIDF vectors
console.log(`â³ Computing TFIDF vectors...`);
const vectorizer = new TFIDFVectorizer(paragraphs, 3000);
const tfidfVectors = vectorizer.vectorizeAll().map(l2normalize);
console.log(`âœ… TFIDF vectors ready.`);

// âœ… Prepare final embeddings
const combinedEmbeddings = unsupEmb; // Start with unsupEmb

// âœ… ELMChain to refine
const chainHiddenUnits = [256, 128];
let embeddings = combinedEmbeddings;

const chainELMs = chainHiddenUnits.map((h, i) =>
    new ELM({
        activation: "relu",
        hiddenUnits: h,
        maxLen: embeddings[0].length,
        categories: [],
        log: { modelName: `IndexerELM_${i}`, verbose: true },
        dropout: 0.02
    })
);

chainELMs.forEach((elm, i) => {
    const path = `./elm_weights/indexer_layer${i}.json`;
    if (fs.existsSync(path)) {
        elm.loadModelFromJSON(fs.readFileSync(path, "utf-8"));
        console.log(`âœ… Loaded Indexer layer ${i}.`);
    } else {
        console.log(`âš™ï¸ Training Indexer layer ${i}...`);
        elm.trainFromData(embeddings, embeddings);
        fs.writeFileSync(path, JSON.stringify(elm.model));
        console.log(`ðŸ’¾ Saved Indexer layer ${i}.`);
    }
    embeddings = elm.computeHiddenLayer(embeddings).map(l2normalize);
});

const indexerChain = new ELMChain(chainELMs);
console.log(`âœ… Indexer chain ready.`);

// âœ… Save embeddings
const embeddingRecords: EmbeddingRecord[] = paragraphs.map((p, i) => ({
    embedding: embeddings[i],
    metadata: { text: p }
}));
fs.writeFileSync("./embeddings/embeddings.json", JSON.stringify(embeddingRecords, null, 2));
console.log(`ðŸ’¾ Saved embeddings.`);

// âœ… Retrieval
function retrieve(query: string, topK = 5) {
    const qVec = encoder.normalize(encoder.encode(query));
    const unsupVec = unsupELM.computeHiddenLayer([qVec])[0];
    const supVec = supELM.computeHiddenLayer([qVec])[0];
    const negVec = negELM.computeHiddenLayer([qVec])[0];

    let combined = [
        ...unsupVec,
        ...supVec,
        ...negVec.map(x => -0.5 * x)
    ];
    combined = l2normalize(combined);

    const finalVec = indexerChain.getEmbedding([combined])[0];
    const tfidfQ = l2normalize(vectorizer.vectorize(query));

    const scored = embeddingRecords.map((r, i) => ({
        text: r.metadata.text,
        dense: r.embedding.reduce((s, v, j) => s + v * finalVec[j], 0),
        tfidf: tfidfVectors[i].reduce((s, v, j) => s + v * tfidfQ[j], 0)
    }));

    return scored
        .sort((a, b) => b.dense - a.dense)
        .slice(0, topK);
}

// âœ… Example retrieval
const results = retrieve("How do you declare a map in Go?");
console.log(`\nðŸ” Retrieval results:`);
results.forEach((r, i) =>
    console.log(
        ` ${i + 1}. (Dense=${r.dense.toFixed(4)})\n${r.text.slice(0, 120)}...`
    )
);
console.log(`\nâœ… Done.`);
